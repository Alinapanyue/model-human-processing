# Signatures of human-like processing in Transformer forward passes

This repository contains code and data for the preprint 
["Signatures of human-like processing in Transformer forward passes"](https://arxiv.org/abs/2504.14107)
by Jennifer Hu, Michael Lepori, and Michael Franke (under review).

The repository is structured as follows:
- `analysis`: contains Jupyter notebooks, R scripts, and outputs from R scripts for data analysis and visualization
- `data`: contains experimental results from humans and models, as well as stimuli
- `figures`: contains rendered figures, generated by notebooks in `analysis/notebooks`
- `scripts`: contains shell scripts
- `src`: contains code for running model experiments

## 1. Evaluating models

### Text-based tasks

To evaluate a model on one of the main text-based tasks, simply run
```bash
bash scripts/run_experiment.sh <MODEL> <TASK>
```

`<MODEL>` should be a Huggingface model identifier (e.g., `meta-llama/Llama-2-7b-hf`). Please see the appendix of our paper for the full list of models used in our experiments.
`<TASK>` should be one of the following:
- `capitals-recall`: Recall of capital cities
- `capitals-recognition`: Recognition (forced choice) of capital cities
- `animals`: Categorization of animal exemplars
- `gender`: Overriding gender biases given contextual cues
- `syllogism`: Syllogistic reasoning

This helper script calls `src/run_experiment.py` under the hood. Please see
the Python file for more details and additional command-line options (e.g., using tuned lens instead of logit lens, or reducing precision to save memory).

All model outputs are saved to the `data/model_output` folder.

### Vision tasks

The code and data for running the vision tasks are contained in the
`src/vision` folder, and largely adapted from 
[bethgelab/model-vs-human](https://github.com/bethgelab/model-vs-human).

## 2. Data analysis

### Combining model and human data
The notebooks `analysis/notebooks/0_process_lm_data.ipynb` and `analysis/notebooks/0_process_vit_data.ipynb` are used to process model outputs for the text-
and vision-based tasks, respectively. These should be run before proceeding
with further analyses, such as model-human comparison. **The final processed files
combining model and human data are saved to `data/human_model_combined`.**

### Fitting regression models

R scripts for fitting mixed-effects regression models and performing model 
comparison for Experiment 2 can be found in the `analysis/r_scripts` folder. 

The results of regression model comparisons are visualized using the notebook 
`analysis/notebooks/3_experiment2.ipynb`.