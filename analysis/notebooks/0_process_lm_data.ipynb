{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042f8edd-4592-49bc-ab7f-b50210f1117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT measures (4): ['output_entropy', 'output_rank_correct', 'output_logprob_correct', 'output_logprobdiff']\n",
      "PROCESS measures (15): ['auc_entropy', 'layer_biggest_change_entropy', 'auc_rank_correct', 'layer_biggest_change_rank_correct', 'auc_logprob_correct', 'layer_biggest_change_logprob_correct', 'auc_logprobdiff_pos', 'auc_logprobdiff_neg', 'layer_biggest_change_logprobdiff', 'auc_boosting_pos', 'auc_boosting_neg', 'layer_argmax_boosting', 'twostage_magnitude', 'twostage_magnitude_latter34', 'twostage_layer']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "sns.set(style=\"ticks\", font_scale=1.2)\n",
    "plt.rc(\"axes.spines\", top=False, right=False)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca8376-abb2-4e5c-87d5-8583bc3692f1",
   "metadata": {},
   "source": [
    "# 0. Helper functions and global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdc5d9",
   "metadata": {},
   "source": [
    "## Helper functions for computing scalar projections (Boosting metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afda5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_VAR = \"correct\"\n",
    "I_VAR = \"incorrect\"\n",
    "\n",
    "def get_logit_diffs(row, abs_val=False,):\n",
    "    correct_logits = row[f\"logits_deltas_{C_VAR}\"]\n",
    "    incorrect_logits = row[f\"logits_deltas_{I_VAR}\"]\n",
    "    if pd.isna(correct_logits) or pd.isna(incorrect_logits):\n",
    "        return None\n",
    "    else:\n",
    "        if isinstance(correct_logits, str):\n",
    "            correct_logits = eval(correct_logits)\n",
    "        if isinstance(incorrect_logits, str):\n",
    "            incorrect_logits = eval(incorrect_logits)\n",
    "        first_correct_logit = correct_logits[0]\n",
    "        first_incorrect_logit = incorrect_logits[0]\n",
    "        if abs_val:\n",
    "            return abs(first_correct_logit) - abs(first_incorrect_logit)\n",
    "        else:\n",
    "            return first_correct_logit - first_incorrect_logit\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors `v1` and `v2`:\n",
    "        >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "        1.5707963267948966\n",
    "        >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "        0.0\n",
    "        >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "        3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ab836-d8c6-41df-ada6-0f862daf71c6",
   "metadata": {},
   "source": [
    "# 1. Read and process model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fabc747-4da6-42cf-901b-a08ff9ad7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_data(task, data_dir=\"../../data/model_output\"):\n",
    "    print(f\"Reading model data for the following task: {task}\")\n",
    "    dfs = []\n",
    "    for lens in [\"logit_lens\", \"tuned_lens\"]:\n",
    "        print(lens)\n",
    "        files = sorted([f for f in listdir(f\"{data_dir}/{lens}\") if f.startswith(task)])\n",
    "        for file in files:\n",
    "            model = file.split(\"_\")[-1].strip(\".csv\")\n",
    "            if model not in N_LAYERS.keys():\n",
    "                print(f\"Skipping model {model}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(f\"{data_dir}/{lens}/{file}\").replace(\"intuitive\", \"incorrect\")\n",
    "            \n",
    "            if I_VAR == \"incorrect\" and \"intuitive\" in df.columns:\n",
    "                continue\n",
    "\n",
    "            assert df.layer_idx.nunique() == N_LAYERS[model]\n",
    "\n",
    "            # Add column for whether the layer is the final layer.\n",
    "            # This is model-dependent.\n",
    "            max_layer = df[\"layer_idx\"].max()\n",
    "            df[\"is_final_layer\"] = df[\"layer_idx\"] == max_layer\n",
    "\n",
    "            # Add normalized layer (between 0-1).\n",
    "            df[\"layer_01\"] = df[\"layer_idx\"] / max_layer\n",
    "\n",
    "            df[\"model\"] = model\n",
    "            df[\"lens\"] = lens\n",
    "            dfs.append(df)\n",
    "        \n",
    "    df = pd.concat(dfs).reset_index().drop(columns=[\"index\"])\n",
    "    \n",
    "    # Add columns for logprob diffs.\n",
    "    for metric in [\"sum\", \"mean\", \"first\"]:\n",
    "        df[f\"{metric}_logprob_diff\"] = (\n",
    "            df[f\"{metric}_logprob_{C_VAR}\"] - df[f\"{metric}_logprob_{I_VAR}\"]\n",
    "        )\n",
    "\n",
    "    # Add columns for logit difference and term difference.\n",
    "    df[\"logit_diff_deltas\"] = df.apply(\n",
    "        lambda r: get_logit_diffs(r, abs_val=False),\n",
    "        axis=1\n",
    "    )\n",
    "    df[\"term_diff_deltas\"] = df.apply(\n",
    "        lambda r: get_logit_diffs(r, abs_val=True),\n",
    "        axis=1\n",
    "    )\n",
    "            \n",
    "    # Add scalar projections.\n",
    "    x = \"term_diff_deltas\"\n",
    "    y = \"logit_diff_deltas\"\n",
    "    norms = np.linalg.norm(df[[x,y]], ord=2, axis=1)\n",
    "    angles = np.array([\n",
    "        angle_between([x,y], [1,1])\n",
    "        for (x,y) in df[[x, y]].values\n",
    "    ])\n",
    "    scalar_projection = np.cos(angles) * norms\n",
    "    df[\"boosting\"] = scalar_projection\n",
    "\n",
    "    # Add column for reciprocal ranks.\n",
    "    for answer_option in [C_VAR, I_VAR]:\n",
    "        df[f\"reciprocal_rank_{answer_option}\"] = 1 / df[f\"rank_{answer_option}_first_token\"]\n",
    "        \n",
    "    # Label syllogism conditions.\n",
    "    if task == \"syllogism\":\n",
    "        df[\"logic_belief_consistent\"] = (\n",
    "            df.is_realistic & \\\n",
    "            (((df.syllogism_condition==\"consistent\")&(df.is_valid)) |\n",
    "            ((df.syllogism_condition==\"violate\")&(~df.is_valid)))\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a65004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model data for the following task: capitals-recall\n",
      "logit_lens\n",
      "tuned_lens\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['term_diff_deltas', 'logit_diff_deltas'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task_dfs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     task: read_model_data(task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m TASKS\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m task_dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapitals-recall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m task_dfs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     task: read_model_data(task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m TASKS\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m task_dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapitals-recall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mread_model_data\u001b[0;34m(task, data_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm_diff_deltas\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_diff_deltas\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(df[[x,y]], \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m angles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     55\u001b[0m     angle_between([x,y], [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x,y) \u001b[38;5;129;01min\u001b[39;00m df[[x, y]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     57\u001b[0m ])\n\u001b[1;32m     58\u001b[0m scalar_projection \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(angles) \u001b[38;5;241m*\u001b[39m norms\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['term_diff_deltas', 'logit_diff_deltas'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "task_dfs = {\n",
    "    task: read_model_data(task=task)\n",
    "    for task in TASKS\n",
    "}\n",
    "task_dfs[\"capitals-recall\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ce5a0-8005-4fa2-9d68-eacf19d651e0",
   "metadata": {},
   "source": [
    "# 2. Compute all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_biggest_change(vals, negative_change=False):\n",
    "    if negative_change:\n",
    "        vals = -vals\n",
    "    changes = [vals[i+1] - vals[i] for i in range(0, len(vals)-1)]\n",
    "    idx = np.argmax(changes)\n",
    "    return idx\n",
    "\n",
    "def area_above_0(vals):\n",
    "    return sum([v for v in vals if v > 0])\n",
    "\n",
    "def area_below_0(vals):\n",
    "    return sum([abs(v) for v in vals if v < 0])\n",
    "\n",
    "def sum_of_neg_vals(vals):\n",
    "    return sum([v for v in vals if v < 0])\n",
    "    \n",
    "def last_neg_index(vals):\n",
    "    reversed_index = next((\n",
    "        i for i, val in enumerate(reversed(vals), 1) if val < 0\n",
    "    ), None)\n",
    "    if reversed_index is None:\n",
    "        return None\n",
    "    else:\n",
    "        return len(vals) - reversed_index\n",
    "\n",
    "def layer_stay_pos(vals):\n",
    "    \"\"\"\n",
    "    Returns 0-indexed layer at which all \n",
    "    subsequent values (including the current one) are positive.\n",
    "    \"\"\"\n",
    "    last_neg = last_neg_index(vals)\n",
    "    if last_neg is None:\n",
    "        # always positive\n",
    "        assert all(v > 0 for v in vals)\n",
    "        return 0\n",
    "    elif last_neg == len(vals)-1:\n",
    "        # the last layer is associated with a negative value\n",
    "        return len(vals)-1\n",
    "    else:\n",
    "        # otherwise, add one layer beyond the last negative layer\n",
    "        return last_neg + 1\n",
    "    \n",
    "def get_auc(vals, iv, vocab_size=None):\n",
    "    if iv == \"entropy\":\n",
    "        return vals.sum()\n",
    "    elif iv == \"rank_correct\":\n",
    "        return (vals - (1/vocab_size)).sum()\n",
    "    elif iv == \"logprob_correct\":\n",
    "        return abs(vals).sum()\n",
    "    elif iv == \"logprobdiff\" or iv == \"boosting\":\n",
    "        pos = area_above_0(vals)\n",
    "        neg = area_below_0(vals)\n",
    "        return pos, neg\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized independent variable: {iv}\")\n",
    "        \n",
    "def get_twostage_metric_magnitude(logprobdiffs, final_logprobdiff):\n",
    "    m = min(logprobdiffs)\n",
    "    if m >= 0:\n",
    "        # If the intuitive answer is never favored, then M = 0.\n",
    "        M = 0\n",
    "    else:\n",
    "        M = min(0, final_logprobdiff) - m\n",
    "    return M\n",
    "        \n",
    "def compute_all_metrics(df, task):\n",
    "    index = ITEM_META_VAR_MAP[task]\n",
    "    if \"model\" not in index:\n",
    "        index = [\"model\"] + index\n",
    "    print(f\"Index for task={task}: {index}\")\n",
    "    \n",
    "    # Get data corresponding to final layer for output measures.\n",
    "    final_layer_df = df[df.is_final_layer].set_index(index)\n",
    "    \n",
    "    df = df.set_index(index)\n",
    "    \n",
    "    results = []\n",
    "    for ind in df.index.unique():\n",
    "        item_data = df.loc[ind].sort_values(by=\"layer_idx\")\n",
    "        # By construction, \"model\" should be the first element of `ind`\n",
    "        model = ind[0]\n",
    "        n_layers = N_LAYERS[model]\n",
    "        # There should be one value per layer\n",
    "        assert len(item_data) == n_layers\n",
    "        \n",
    "        # ~~~~~~~~~~~~~~~~~~~~~ (0) Initialize metadata about the stimulus.\n",
    "        meta_data = {k: item_data[k].values[0] for k in TASK_META_VAR_MAP[task]}\n",
    "        for i, index_var in enumerate(index):\n",
    "            meta_data[index_var] = ind[i]\n",
    "            \n",
    "        # ~~~~~~~~~~~~~~~~~~~~~ (1) Add static metrics from single layers. (\"output metrics\").\n",
    "        static_data = {}\n",
    "        for output_iv in OLD_OUTPUT_IVS:\n",
    "            # FINAL layer baseline\n",
    "            new_iv = OUTPUT_IV_MAP[output_iv]\n",
    "            static_data[new_iv] = final_layer_df.loc[ind][output_iv]\n",
    "\n",
    "            # MIDPOINT layer. baseline\n",
    "            new_iv = OUTPUT_IV_MAP[output_iv].replace(\"output_\", \"midpoint_\")\n",
    "            midpoint = int(N_LAYERS[model]/2) - 1\n",
    "            static_data[new_iv] = item_data[item_data.layer_idx==midpoint].loc[ind][output_iv]\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~ (2) Compute trajectory-based metrics.\n",
    "        process_data = {}\n",
    "        \n",
    "        for var_name, clean_var_name in list(OUTPUT_IV_MAP.items()) + [(\"boosting\", \"boosting\")]:\n",
    "            # Get base name of metric. Should be one of the following: \n",
    "            # \"entropy\", \"rank_correct\", \"logprob_correct\", \"logprobdiff\"\n",
    "            iv = clean_var_name.replace(\"output_\", \"\")\n",
    "            \n",
    "            # Add AUC measure(s).\n",
    "            data = item_data.sort_values(by=\"layer_idx\")[var_name]\n",
    "            auc = get_auc(data, iv, vocab_size=get_vocab_size(model))\n",
    "            if iv == \"logprobdiff\" or iv == \"boosting\":\n",
    "                auc_pos, auc_neg = auc\n",
    "                process_data[f\"auc_{iv}_pos\"] = auc_pos\n",
    "                process_data[f\"auc_{iv}_neg\"] = auc_neg\n",
    "            else:\n",
    "                process_data[f\"auc_{iv}\"] = auc\n",
    "\n",
    "            # Add biggest change measure.\n",
    "            if iv == \"boosting\":\n",
    "                process_data[f\"layer_argmax_{iv}\"] = np.argmax(data)\n",
    "            else:\n",
    "                layer_biggest_change = get_index_of_biggest_change(\n",
    "                    data,\n",
    "                    negative_change=(iv == \"entropy\")\n",
    "                )\n",
    "                process_data[f\"layer_biggest_change_{iv}\"] = layer_biggest_change\n",
    "                \n",
    "            # Add dual-processing metrics (CoM and TTD).\n",
    "            # =========== CoM (\"change of mind\"; magnitude-based)\n",
    "            final_logprobdiff = final_layer_df.loc[ind][\"first_logprob_diff\"]\n",
    "            M = get_twostage_metric_magnitude(\n",
    "                item_data[\"first_logprob_diff\"], \n",
    "                final_logprobdiff\n",
    "            )\n",
    "            process_data[\"twostage_magnitude\"] = M\n",
    "            # restricted to final 3/4 layers (~NOT USED IN THE PAPER~)\n",
    "            threshold = int(n_layers/4)\n",
    "            M_34 = get_twostage_metric_magnitude(\n",
    "                item_data[\"first_logprob_diff\"].values[threshold:], \n",
    "                final_logprobdiff\n",
    "            )\n",
    "            process_data[\"twostage_magnitude_latter34\"] = M_34\n",
    "            \n",
    "            # =========== TTD (\"time to decision\"; time-based)\n",
    "            process_data[\"twostage_layer\"] = (\n",
    "                (layer_stay_pos(item_data[\"first_logprob_diff\"])+1) / n_layers\n",
    "            )\n",
    "        \n",
    "        # ~~~~~~~~~~~~~~~~~~~~~ Combine all data.\n",
    "        res = meta_data | static_data | process_data\n",
    "        results.append(res)\n",
    "        \n",
    "    results = pd.DataFrame(results)\n",
    "    \n",
    "    if task == \"animals\":\n",
    "        print(\"Adding German annotations\")\n",
    "        # Annotate with the original German names, for compatibility with human data.\n",
    "        stims = pd.read_csv(\"../../data/stimuli/animals.csv\").set_index(\"exemplar\")\n",
    "        results[\"exemplar_de\"] = results.exemplar.apply(\n",
    "            lambda e: stims.loc[e][\"exemplar_de\"]\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfae5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute all model predictor metrics.\n",
    "task_metrics = defaultdict(dict)\n",
    "for task, df in task_dfs.items():\n",
    "    print(\"=\"*80)\n",
    "    print(task)\n",
    "    print(\"=\"*80)\n",
    "    for lens in [\"logit_lens\", \"tuned_lens\"]:\n",
    "        print(lens)\n",
    "        metrics = compute_all_metrics(df[df.lens==lens], task)\n",
    "        metrics.to_csv(\n",
    "            f\"../../data/model_output/processed/{task}_metrics_{lens}.csv\", index=False\n",
    "        )\n",
    "        task_metrics[task][lens] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9ac9f",
   "metadata": {},
   "source": [
    "# 4. Combine with human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4a95a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def zscore_col(df, col, group=\"subject_id\"):\n",
    "    ppt_means = df.groupby(group)[col].mean()\n",
    "    ppt_stds = df.groupby(group)[col].std()\n",
    "    df[f\"{col}_zscore\"] = df.apply(\n",
    "        lambda r: (r[col]-ppt_means.loc[r[group]]) / ppt_stds.loc[r[group]],\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def read_human_data(task):\n",
    "    if task == \"capitals-recall\":\n",
    "        trial_df = pd.read_csv(f\"../../data/human/{task}_trial_labeled.csv\")\n",
    "    else:\n",
    "        trial_df = pd.read_csv(f\"../../data/human/{task}_trial.csv\")\n",
    "        if task == \"animals\":\n",
    "            trial_df = trial_df.rename(columns={\n",
    "                \"correct\": \"response_correct\",\n",
    "                \"Exemplar\": \"exemplar_de\"\n",
    "            })\n",
    "            # Add z-scored RTs within participant.\n",
    "            trial_df = zscore_col(trial_df, \"RT\", group=\"subject_nr\")\n",
    "    print(f\"Read human data for task = {task}\")\n",
    "    print(\"Num rows in human data:\", len(trial_df))\n",
    "    return trial_df\n",
    "\n",
    "def combine_model_human_data(model_df, trial_df, task):\n",
    "    assert model_df.model.nunique()==1\n",
    "\n",
    "    # Drop columns that are all NaN. This happens for smaller models,\n",
    "    # where there is no value for e.g. intermediate_layer_124_entropy\n",
    "    model_df = model_df.dropna(axis=1, how='all')\n",
    "\n",
    "    # Grab the IVs by excluding all meta variables.\n",
    "    meta_vars = TASK_META_VAR_MAP[task] + ITEM_META_VAR_MAP[task]\n",
    "    ivs = [c for c in model_df.columns if not c in meta_vars]\n",
    "    \n",
    "    if task.startswith(\"capitals\"):\n",
    "        index = \"entity\"\n",
    "    elif task == \"syllogism\":\n",
    "        index = \"unique_id\"\n",
    "    elif task == \"animals\":\n",
    "        index = \"exemplar_de\"\n",
    "\n",
    "    # Add model-derived measures to human trial-level data.\n",
    "    # For capitals-recall and animals there will only be one value,\n",
    "    # but for capitals-recognition and syllogism we need to take the mean \n",
    "    # across the two test conditions (correct_first and intuitive_first,\n",
    "    # or the two orderings of premises in the argument)\n",
    "    model_means = model_df.groupby(index)[ivs].mean().reset_index()\n",
    "    trial_df = trial_df.merge(model_means, on=index)\n",
    "\n",
    "    return trial_df\n",
    "\n",
    "\n",
    "for task, lens_metrics in task_metrics.items():\n",
    "    print(\"=\"*80)\n",
    "    print(task)\n",
    "    print(\"=\"*80)\n",
    "    # Read human trial-level data.\n",
    "    try:\n",
    "        human_data = read_human_data(task=task)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No human data found for {task}; skipping...\")\n",
    "        continue\n",
    "    for lens, metrics in lens_metrics.items():\n",
    "        # Combine with data from each model.\n",
    "        for model in metrics.model.unique():\n",
    "            print(lens, model)\n",
    "            trials = combine_model_human_data(metrics[metrics.model==model], human_data, task=task)\n",
    "            trials.to_csv(\n",
    "                f\"../../data/human_model_combined/{lens}/{task}_{model}.csv\", \n",
    "                index=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de41ed-c1b3-4b6c-8331-1408112de9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
